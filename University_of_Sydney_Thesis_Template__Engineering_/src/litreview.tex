\chapter{Literature review\label{cha:litreivew}}

Text.

\section{Synapse Detection \& Segmentation}

Synapse detection and segmentation in volume electron microscopy constitute a foundational stage for reliable connectome reconstruction. In this section, the focus is explicitly on electron‐microscopy data, with an emphasis on \emph{serial-section electron microscopy} (ssEM) and \emph{focused ion beam scanning electron microscopy} (FIB--SEM). We formalize a three-tier objective: (i) \emph{synaptic site or cleft detection}, that is, localizing presynaptic and postsynaptic specializations or the interposed cleft; (ii) \emph{instance-level or boundary-aware segmentation} of synaptic structures, including active zones, postsynaptic densities, and vesicle clusters, at voxel or supervoxel resolution; and (iii) \emph{partner assignment} as an optional stage that links each detected site to its presynaptic and postsynaptic neurites, thereby instantiating edges in the neural graph. Pioneering random-forest and context-cue pipelines established this decomposition on anisotropic ssEM stacks and highlighted the need for explicit structural priors (\cite{Kreshuk2011PLOS,Becker2012MICCAI,Becker2013TMI,Kreshuk2014PLOS}). Practical deployment is complicated by extreme class imbalance and the small, sparse, and morphologically diverse nature of synapses; anisotropy in ssEM and contrast drift across large volumes; weak or partial supervision (points, line segments, or sparse cleft masks) that limits dense annotation; and pronounced domain shift across species, brain regions, staining protocols, and acquisition sites. Large-scale pipelines subsequently scaled classical detectors with deep feature learners and graph consolidation to sustain throughput on mammalian connectomes (\cite{Huang2014DAWMR,Roncal2015VESICLE,Li2018BMC}). In addition, segmentation and assignment errors propagate topologically into neuron-level reconstructions and downstream graph analyses, while constraints on compute and human proofreading cost impose requirements on throughput, calibration, and robustness. Deep synapse detectors matured into production-ready systems for proofreading and fluorescence-assisted workflows, underscoring the demand for calibrated outputs and quality assurance (\cite{Staffler2017SynEM,Simhal2017PCBiol,Huang2018FullAuto}). Signed-proximity estimation and attention-based partner assignment reframed linkage as a learning problem that couples detection with graph inference (\cite{Parag2018SignedProximity,Turner2019AVAN}). Augmented architectures such as CleftNet, Synful, and cerebellar-specific pipelines demonstrated anisotropy-aware design choices and large-scale automation (\cite{Liu2021CleftNet,Buhmann2021Synful,Park2022Cerebellar}). Recent advances extend to cross-domain adaptation, dual-channel or instance segmentation, and species transfer, motivating the unified blueprint summarized below (\cite{Svara2022Zebrafish,Chen2023AdaSyn,Chen2024DualChannel,Liu2024InstanceSeg}). Emerging whole-brain connectomes and generalized detectors further emphasize scalable generalization and cross-lab reproducibility (\cite{Yu2025NewSynapse,Mohinta2025SimpSyn}). These considerations motivate methods that integrate multi-scale context, enforce structural consistency, and generalize across datasets.

\noindent\textbf{Roadmap.} The remainder of this section follows a common template (definition $\rightarrow$ methods $\rightarrow$ evaluation $\rightarrow$ failure modes $\rightarrow$ emerging trends) and is organized into six parallel subsections: \emph{Problem scope and data regime}, which clarifies inputs, outputs, supervision regimes, and noise characteristics; \emph{Classical pipelines}, which cover feature-engineered detectors, Markov and conditional random fields, and supervoxel or graph-based post-processing; \emph{Deep learning paradigms}, which span two- and three-dimensional U-Net families, transformer backbones, two-stage candidate-to-refinement pipelines, and end-to-end instance models with uncertainty estimation; \emph{Structural priors and constraints}, which include biological context, connectivity- and topology-preserving objectives, and multi-task consistency; \emph{Benchmarks, metrics and failure modes}, which unify detection, segmentation, topology, and graph-level metrics while cataloguing typical errors; and \emph{Trends and open issues}, which highlight integrated detect–segment–assign objectives, cross-domain self-supervision, and interactive correction workflows.

\subsection{Problem scope and data regime}

\noindent\textbf{Task definition}\;
This subsection formalizes synapse analysis in volume electron microscopy with emphasis on serial section electron microscopy (ssEM) and focused ion beam scanning electron microscopy (FIB--SEM). We adopt a three-tier objective that maps voxels to synaptic structures and then to edges in a connectome:
(i) \emph{synaptic site or cleft detection}, i.e., localizing presynaptic specializations, postsynaptic densities, or the interposed cleft;
(ii) \emph{instance-level or boundary-aware segmentation} of synaptic components (active zones, postsynaptic densities, vesicle clusters);
(iii) \emph{partner assignment} that links each detected site to its presynaptic and postsynaptic neurites to instantiate synaptic edges.
Unless stated otherwise, polyadic synapses (one presynaptic site with many postsynaptic partners) are allowed; self-synapses and inhibitory/excitatory types are not distinguished at the label level in this section.\par

\medskip
\noindent\textbf{Definition and notation}\;
Let the EM volume be \(x \in \mathbb{R}^{H \times W \times D}\) with voxel spacings \(\Delta=(\Delta_x,\Delta_y,\Delta_z)\) in nanometres.
A voxel index \(\mathbf r=(i,j,k)\) corresponds to the physical coordinate \(\tilde{\mathbf r}=(i\Delta_x,\,j\Delta_y,\,k\Delta_z)\).
We denote the set of site candidates by \(\mathcal P\), the synapse mask by \(S \subset \{1,\dots,H\}\times\{1,\dots,W\}\times\{1,\dots,D\}\), and presynaptic/postsynaptic subsets by \(\mathcal P_{\mathrm{pre}}\) and \(\mathcal P_{\mathrm{post}}\).
The partner set is \(\mathcal E \subseteq \mathcal P_{\mathrm{pre}}\times \mathcal P_{\mathrm{post}}\).
All geometric quantities (distances, radii, areas, volumes) are stated in \emph{physical units}.
We view the three tasks as
\[
\text{Detection: } x \mapsto \mathcal P,\qquad
\text{Segmentation: } x \mapsto S,\qquad
\text{Assignment: } (\mathcal P,S) \mapsto \mathcal E.
\]\par

\medskip
\noindent\textbf{Inputs}\;
The input consists of volumetric EM acquired under two prevalent regimes.
ssEM typically exhibits pronounced anisotropy along the sectioning axis (\(\Delta_z \gg \Delta_x,\Delta_y\)), which complicates 3D context aggregation and boundary continuity.
FIB--SEM approaches near-isotropic sampling, improving volumetric continuity while sometimes offering lower in-plane contrast for fine ultrastructure relative to ssEM.
In selected settings, light microscopy and correlative LM--EM provide auxiliary cues for registration, cross-modal consistency checks, or weak supervision (\cite{Simhal2017PCBiol}).\par

\medskip
\noindent\textbf{Preprocessing and tiling (for reproducibility)}\;
To standardize inputs and enable scalable inference, we assume: global or per-tile intensity normalization; optional denoising; slice-to-slice alignment for ssEM; artifact masking (folds, cracks, missing sections); and resampling if needed for network stride compatibility.
Block-wise inference uses fixed window size, stride, and overlap with consensus reconciliation on overlaps; these values are reported in physical units.\par

\medskip
\noindent\textbf{Outputs and matching policy}\;
We distinguish three outputs that may be produced jointly or sequentially.
\emph{Detection} yields point-like sites or small spherical/box proposals.
\emph{Segmentation} returns voxel masks for synaptic compartments, with optional membrane-contact constraints.
\emph{Partner assignment} produces a set of edges \(\mathcal E\) with one-to-one, one-to-many (polyadic), or many-to-many cardinalities, as predefined in protocol.
Let \( d_{\mathrm{phys}}(p,g)=\|\tilde{\mathbf r}(p)-\tilde{\mathbf r}(g)\|_2 \).
Detection matches are declared if \(d_{\mathrm{phys}}(p,g)\le r\) with a default tolerance radius \(r\) specified in nanometres and accompanied by a sensitivity sweep.
Partner legality requires (i) nonzero membrane contact between presynaptic and postsynaptic neurites; and (ii) thresholds on minimum contact area and maximum centroid distance, both in physical units.\par

\medskip
\noindent\textbf{Supervision regimes}\;
Annotation is frequently incomplete and heterogeneous: points, short line segments tracing clefts or active zones, or sparse masks for selected subvolumes.
Many datasets provide weakly supervised local blocks rather than globally dense labels, and cross-laboratory or cross-species scenarios often provide no target-domain labels (\cite{Park2022Cerebellar}).
Learning must therefore contend with severe class imbalance and a preponderance of very small objects (\cite{Svara2022Zebrafish}).\par

\medskip
\noindent\textbf{Noise characteristics and acquisition drift}\;
Real-world volumes show contrast fluctuations across tiles or sessions, section misalignment and residual warping (ssEM), staining artefacts, stripe noise or deposition, focus drift, and resampling-induced deformations from alignment or block merging.
These factors distort local texture statistics and long-range context, undermining both candidate generation and boundary closure; robust training typically includes augmentation and quality-gating for these artefacts.\par

\medskip
\noindent\textbf{Interfaces to downstream tasks}\;
Synapse analysis is coupled to neuron segmentation and to graph construction.
Synapse mis-localization can induce false splits or merges in neurite agglomeration, while neuron-mask errors can misassign partners; both effects propagate topologically and alter circuit motifs and path lengths.
Edge existence and weight estimation depend on partner assignment and on calibrated detection scores; miscalibration at the detector level induces systematic errors in graph-level metrics (degree distributions, modularity, motif counts).
Evaluations should therefore consider voxel/instance accuracy alongside graph fidelity and human proofreading cost.\par

\medskip

\subsection{Classical pipelines}

\noindent\textbf{Scope and three-stage layout}\;
Classical (pre-deep) synapse pipelines typically follow a three-stage design:
(i) \emph{preprocessing \& candidate detection} (templates/filters/morphology, e.g., DoG, normalized cross-correlation, anisotropic kernels);
(ii) \emph{segmentation \& component extraction} (thresholding \(+\) connected components, watershed, active contours, superpixel/supervoxel graphs);
(iii) \emph{structured inference} (MRF/CRF, \(s\)-\(t\) graph cuts, shortest paths, or integer programming).
This subsection focuses on these canonical designs and their reproducible details; end-to-end deep models are discussed elsewhere.

\medskip
\noindent\textbf{Preprocessing and normalization}\;
Common steps include slice-to-slice registration (ssEM), denoising (nonlocal means, BM3D, anisotropic diffusion), intensity standardization (histogram matching or per-tile normalization), and artefact masking (folds, cracks, missing sections).
Block-wise inference specifies window size, stride, and overlap; cross-block reconciliation uses max/avg fusion or non-maximum suppression (NMS).
All sizes are reported in physical units.

\medskip
\noindent\textbf{Hand-crafted feature families}\;
Given voxel intensities \(I(\mathbf r)\) and scale-space \(I_\sigma = G_\sigma * I\),
\[
g_\sigma(\mathbf r)=\lVert \nabla I_\sigma(\mathbf r)\rVert_2,\qquad
\Delta I_\sigma,\qquad
\mathbf H_\sigma=\nabla^2 I_\sigma.
\]
Features are grouped as \emph{intensity/contrast}, \emph{gradient/edge} \((g_\sigma,\,\Delta I_\sigma)\), \emph{texture} (Gabor, LM filter bank, LBP), \emph{morphology} (opening/closing, distance transform, skeletons), and \emph{structure-tensor/Hessian} cues for sheet-/tube-like responses.
A typical sheet-response uses the Hessian eigenvalues \(\lambda_1\le\lambda_2\le\lambda_3\):
\[
R_{\text{sheet}}
=\exp\!\Big(-\tfrac{\lambda_1^2+\lambda_2^2}{2\alpha^2}\Big)\cdot
\Big(1-\exp\!\big(-\tfrac{\lambda_3^2}{2\beta^2}\big)\Big).
\]

\medskip
\noindent\textbf{Candidate detection: DoG and NCC}\;
Difference-of-Gaussians (DoG) at scales \(\sigma_1,\sigma_2\):
\[
D(\mathbf r;\sigma_1,\sigma_2)=(G_{\sigma_1}-G_{\sigma_2})*I,\qquad
\mathbf r^\star=\arg\max_{\mathbf r} D(\mathbf r;\cdot)\ \ \text{s.t.}\ \ D>\tau.
\]
Normalized cross-correlation (NCC) with template \(T\) over window \(\Omega\):
\[
\mathrm{NCC}(\mathbf r)=
\frac{\sum_{\mathbf u\in\Omega}\!\big(I(\mathbf r+\mathbf u)-\bar I_\Omega\big)\big(T(\mathbf u)-\bar T\big)}
{\sqrt{\sum_{\mathbf u\in\Omega}\!\big(I(\mathbf r+\mathbf u)-\bar I_\Omega\big)^2}\,
 \sqrt{\sum_{\mathbf u\in\Omega}\!\big(T(\mathbf u)-\bar T\big)^2}}.
\]
Peaks are selected across multiple scales with anisotropic kernels (heavier \(z\)-weights for ssEM) and pruned by NMS.

\medskip
\noindent\textbf{Segmentation models: from thresholding to structured inference}\;
Starting from thresholding \(+\) connected components or watershed seeds, classical methods lift decisions into graphical models.
For binary labels \(y_i\in\{0,1\}\) (non-synapse/synapse), a standard CRF/MRF energy is
\[
E(\mathbf y)=\sum_i \psi_i(y_i\,|\,x)+\sum_{(i,j)\in\mathcal E}\psi_{ij}(y_i,y_j),
\]
with unary \(\psi_i=-\log P(y_i\,|\,\phi(I))\) from hand-crafted features \(\phi\),
and pairwise Potts \(\psi_{ij}=\lambda\,\mathbf 1[y_i\neq y_j]\cdot w_{ij}\) where \(w_{ij}\) depends on gradient, boundary, or membrane likelihood.
If \(\psi_{ij}\) is a metric Potts, the optimum is found via \(s\)-\(t\) graph cuts; otherwise, \(\alpha\)-expansion or loopy BP gives approximate inference.
Shortest-path stitching and region-merging provide alternatives when thin interfaces fragment.

\medskip
\noindent\textbf{Supervoxels and region adjacency graphs (RAG)}\;
Watershed or SLIC yields supervoxels as nodes of a graph \(\mathcal G=(\mathcal V,\mathcal E)\).
Edge weights \(w_{uv}\) combine boundary strength, average gradient, texture disparity, or membrane probability.
Bottom-up merging applies a criterion
\[
\Delta \mathcal J=\Delta\mathrm{Data}-\gamma\,\Delta\mathrm{Boundary},
\quad \text{merge if }\Delta \mathcal J>\tau_{\text{merge}},
\]
with sparse \(z\)-linking for anisotropic stacks.

\medskip
\noindent\textbf{Active contours / level sets (optional)}\;
Energy consists of a data term and length/curvature regularization; evolution proceeds until a stopping rule (e.g., small update norm) is met.
They are useful when shapes are smooth but edges are weak, and can be seeded from candidate sites.

\medskip
\noindent\textbf{Topology/bio priors as post-processing}\;
Rules include membrane-contact constraints (reject non-membrane candidates), minimum contact area and maximum inter-centroid distance, hole filling and thin-layer preservation (morphological repair), and 3D connectivity consistency across slices.

\medskip
\noindent\textbf{Strengths and limitations}\;
\begin{itemize}\setlength\itemsep{0.25em}
  \item \emph{Strengths:} interpretability of features/energies; low label demand; fast training and modest memory; straightforward rule injection (smoothness, contiguity, simple geometric priors).
  \item \emph{Limitations:} weak cross-domain generalization; limited capacity for context and topology (polyadic relations, membrane–cleft–vesicle dependencies); difficulty with multi-scale and very small targets; cascading errors between candidate and consolidation stages; calibration often suboptimal for proofreading cost.
\end{itemize}

\medskip
\noindent\textbf{Typical instantiations and scenarios}\;
\begin{itemize}\setlength\itemsep{0.25em}
  \item \emph{Texture \(+\) MRF for candidate generation.} Multiscale textures or steerable filters scored by RF/SVM, followed by MRF/CRF smoothing, suit small anisotropic ssEM volumes with scarce labels and moderate cleft contrast.
  \item \emph{Supervoxel posteriors \(+\) graph consolidation.} Watershed/SLIC on membrane/cleft likelihoods, then graph cuts or shortest paths to fuse fragments, supports batch processing of small subvolumes and manual curation.
  \item \emph{Low-annotation regimes.} Random-forest scoring of putative clefts or T-bar–like structures \(+\) CRF remains a pragmatic baseline when only points/lines/sparse masks are available or for rapid prototyping.
\end{itemize}

\medskip
\noindent\textbf{Parameter selection and ablations}\;
Report thresholds, scale sets \(\{\sigma\}\), template sizes, NMS radius, Potts weight \(\lambda\), edge-weight construction, RAG merge threshold \(\tau_{\text{merge}}\), shortest-path costs, and contact-area/distance thresholds (all in physical units).
Use grid or Bayesian search; provide sensitivity curves and cross-validation design.

\medskip
\noindent\textbf{Evaluation protocol (aligned with later sections)}\;
Detection uses a physical matching radius \(r\) (default \(r=\)\textit{XX}\,nm; include \(r\)-sweep).
Segmentation reports Dice, VOI, Rand \(F\); boundary reports Boundary-F1 with tolerance \(\delta=\)\textit{YY}\,nm.
Provide PR/AP for candidates, instance-level \(F_1\), and, if applicable, graph consistency.
Attach 95\% confidence intervals (bootstrap over instances/tiles) and hardware/run-time/memory.

\medskip
\noindent\textbf{Runtime and resources}\;
State throughput (voxels/s), peak memory, single-/multi-core speedups, and parallelism (tiling/pipelining); specify hardware and implementation language.

\medskip
\noindent\textbf{Failure modes and boundary conditions}\;
Common issues include staining drift, strong artefacts, extreme anisotropy, low contrast, and confounds from vesicles or myelin, leading to false positives/negatives.
Mitigations include adaptive thresholds, region rejection, and local re-estimation around hard cases.

\medskip
\noindent\textbf{Interface to deep methods}\;
Classical modules provide priors, pseudo-labels, and post-processors for deep systems (e.g., CRF/graph-cut refinement), and serve as weak supervision or fallbacks in low-label regimes.

\medskip
\noindent\textbf{Algorithm C1: reference classical pipeline}\;
\begin{enumerate}
  \item \textbf{Preprocess}: registration \(\rightarrow\) denoise \(\rightarrow\) intensity standardization \(\rightarrow\) artefact masking.
  \item \textbf{Candidate detection}: DoG/NCC/morphology (multi-scale, anisotropic) \(\rightarrow\) NMS for points/patches.
  \item \textbf{Feature extraction}: compute \(\phi(I)\) (intensity/texture/gradient/Hessian/morphology).
  \item \textbf{Unary scoring}: train/apply RF/SVM/AdaBoost to obtain \(P(y_i\,|\,\phi)\).
  \item \textbf{Structured segmentation}: CRF/graph-cuts/watershed\(+\)RAG on candidate neighborhoods or full volume.
  \item \textbf{Topology-aware postprocess}: membrane-contact checks, contact-area/distance thresholds, hole repair, 3D connectivity.
  \item \textbf{Threshold \& calibration}: PR/ROC sweeps; choose operating point; optional temperature/Platt scaling.
  \item \textbf{Reporting}: metrics ( \(r,\delta\), Dice/VOI/RandF/AP ), run-time/memory, sensitivity and ablations.
\end{enumerate}

\medskip
\begin{table}[t]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{2pt}%
  \renewcommand{\arraystretch}{1.05}%
  \begin{tabular}{@{} p{0.15\linewidth} p{0.21\linewidth} p{0.17\linewidth} p{0.20\linewidth} p{0.22\linewidth} @{}}
    \hline
    \textbf{Family} & \textbf{Operators / models} & \textbf{Priors / constraints} & \textbf{Advantages} & \textbf{Limitations / failures} \\
    \hline
    Templates / filtering &
    DoG, LoG, NCC, Gabor, morphology &
    Multi-scale, anisotropic kernels &
    Simple; fast; light memory &
    Sensitive to contrast/protocol shifts; threshold dependent \\
    Shallow classifiers &
    RF / SVM / AdaBoost on \(\phi(I)\) (intensity/gradient/texture/Hessian) &
    Class weights; imbalance handling &
    Interpretable; controllable; small label demand &
    Hand-crafted features needed; weak domain transfer \\
    Structured models &
    CRF/MRF (Potts), graph cuts, shortest paths, integer programming &
    Smoothness; boundary; membrane-contact priors &
    Good boundary consistency; topology control &
    Many hyperparameters; approximate inference; tuning cost \\
    Region methods / RAG &
    Watershed\(+\)RAG; hierarchical merging; MST-style agglomeration &
    Intra-region coherence; boundary penalty; sparse \(z\)-links &
    Enables instance merging; scalable on blocks &
    Seed/damping sensitive; over-merge risk; seed bias \\
    Active contours / level sets &
    Chan--Vese; geodesic AC; curvature regularization &
    Length/curvature priors; shape bias &
    Robust to weak edges with proper seeds &
    Leakage under strong noise; parameter sensitive \\
    \hline
  \end{tabular}
  \caption{Classical method families in synapse analysis. Keep physical units for any thresholds; insert concrete implementations and citations in the camera-ready version.}
  \label{tab:classical-compare}
\end{table}

\subsection{Deep learning paradigms}

\noindent\textbf{Scope and task decomposition}\;
This subsection covers deep-learning paradigms for three coupled goals:
\emph{detection} (point/heatmap/center-offset representations),
\emph{segmentation} (semantic/instance/boundary-aware masks),
and \emph{partner assignment} (pre--post linkage and graph edges).
Compared with classical three-stage pipelines, deep systems may be \emph{end-to-end} (joint heads for all goals) or \emph{two-stage} (candidate proposal \(\rightarrow\) refinement/post-processing), while retaining clear interfaces to classical modules when desired.\par

\medskip
\noindent\textbf{Network archetypes and anisotropy-aware design}\;
We group backbones into four families and make the anisotropy choices explicit for ssEM versus near-isotropic FIB--SEM:
\begin{itemize}
  \item \emph{2D/2.5D U-Net family}: in-plane convolutions with slice stacks (2.5D) or occasional axial \(1\times 1\times k\) kernels; memory-efficient, high throughput; limited axial context.
  \item \emph{3D U-Net / hybrid 2.5D}: anisotropic kernels \(k\times k\times 1\) in early stages and delayed \(z\)-downsampling; stronger 3D context; higher memory and more complex tiling.
  \item \emph{CNN+Transformer hybrids} (e.g., windowed in-plane attention with axial gating): large effective receptive fields and cross-domain robustness; training stability and memory must be managed.
  \item \emph{Multi-head multi-task decoders} (FPN/DeepLab-like): shared encoder with parallel heads for detection/segmentation/boundary/pairing; enables end-to-end coupling but requires careful loss balancing.
\end{itemize}
For strongly anisotropic ssEM, we use mixed kernel shapes (e.g., \(k\times k\times 1\) interleaved with \(1\times 1\times k\)), delay \(z\)-strides until deeper levels, or apply axial attention/gating to capture long-range slice structure.\par

\medskip
\noindent\textbf{Heads for the three goals}\;
\emph{Detection (anchor-free preferred)}: a center heatmap \(\hat{Y}^{\mathrm{det}}\) and offsets \(\hat{O}\) (optionally size/radius), peak finding \(\rightarrow\) NMS with a \emph{physical} suppression radius; in crowded tiny-object regimes, soft-NMS or weighted fusion reduces mutual suppression.
\emph{Segmentation}: semantic logits \(\hat{S}\) (Dice/CE/Tversky objectives) and optional boundary head \(\hat{B}\); instance masks via center-offset clustering, energy/level-set proxy, or proposal-based heads.
\emph{Partner assignment}: embed pre/post candidates into \(\hat{Z}\) (metric learning) or predict pairwise affinities \(\hat{A}\); training uses local \emph{soft-assignment} within a radius \(r\), while inference may apply Hungarian or min-cost flow, with membrane-contact and distance/area rules.\par

\medskip
\noindent\textbf{Losses and balancing (formalized)}\;
\emph{Detection \& segmentation}:
\[
L_{\text{det}}
=\mathrm{FL}\big(\hat{Y}^{\text{det}},Y^{\text{det}}\big)
+\beta\,\lVert \hat{O}-O\rVert_{1,\mathcal{N}},\qquad
L_{\text{seg}}
=(1-\mathrm{Dice}(\hat{S},S))+\alpha\,\mathrm{CE}(\hat{S},S)+\eta\,L_{\text{bd}},
\]
where \(L_{\text{bd}}\) regularizes boundaries (e.g., distance-transform bands of width \(\delta\) or a level-set proxy).
\emph{Pairing (two options)}:
\[
\text{(embeddings)}\quad
L_{\text{pair}}
=\sum_{(i,j)\in\mathcal{N}_r}\ell_{\text{pos}}(\hat{Z}_i,\hat{Z}_j)
+\sum_{(i,k)\notin\mathcal{N}_r}\ell_{\text{neg}}(\hat{Z}_i,\hat{Z}_k),
\]
\[
\text{(affinities)}\quad
L_{\text{pair}}
=-\!\!\sum_{(i,j)\in\mathcal{N}_r}\!\!\log\sigma(\hat{A}_{ij})
-\!\!\sum_{(i,k)\notin\mathcal{N}_r}\!\!\log\!\big(1-\sigma(\hat{A}_{ik})\big),
\]
where \(\mathcal{N}_r\) collects candidate pairs within physical radius \(r\).
\emph{Topological/biological priors (optional)}:
\[
L_{\text{topo}}
=\lambda_{\mathrm{conn}}\mathcal{L}_{\mathrm{conn}}(\hat{S})
+\lambda_{\mathrm{hole}}\mathcal{L}_{\mathrm{hole}}(\hat{S})
+\lambda_{\mathrm{mc}}\mathcal{L}_{\mathrm{membrane\_contact}}(\hat{S},M),
\]
with \(M\) a membrane probability/mask, implemented via soft-skeleton or distance transforms.
\emph{Calibration}:
\[
\hat{p}^{(T)}=\mathrm{softmax}(z/T),\qquad
\mathrm{ECE}=\sum_{b=1}^{B}\frac{n_b}{n}\,\bigl|\mathrm{acc}(b)-\mathrm{conf}(b)\bigr|.
\]
\emph{Total loss}:
\[
L_{\text{total}}
=\lambda_{\text{det}}L_{\text{det}}
+\lambda_{\text{seg}}L_{\text{seg}}
+\lambda_{\text{pair}}L_{\text{pair}}
+\lambda_{\text{topo}}L_{\text{topo}}
+\lambda_{\text{cal}}\mathrm{ECE},
\]
with \(\lambda_{\cdot}\) chosen by grid search, uncertainty weighting, or GradNorm, and sensitivity reported.\par

\medskip
\noindent\textbf{Anisotropy and 2.5D specifics (quantified)}\;
When \(\Delta_z\gg \Delta_x,\Delta_y\): use kernel schedules such as early \(k\times k\times 1\) (no \(z\)-downsampling) then \(k\times k\times 2\) at depth \(N\); adopt slice stacks of depth \(d\) in 2.5D encoders; and include axial attention/gating with window \(k\) to keep memory bounded. Downstream NMS and pairing radii are specified in \emph{physical} units to avoid voxel-size confounds.\par

\medskip
\noindent\textbf{Augmentation, semi-supervision, and domain adaptation}\;
Intensity (brightness/contrast/gamma), histogram matching, \emph{missing-slice simulation}, in-plane rotations/flips, elastic warps, light scaling, cutout/cutmix, and EM-like noise injection.
For semi-supervision, adopt teacher--student consistency (weak/strong views) with confidence filtering and simple morphology checks; for domain adaptation, combine histogram/style transfer with feature alignment (e.g., MMD/adversarial) as lightweight options.\par

\medskip
\noindent\textbf{Calibration and uncertainty for QA}\;
Apply temperature scaling or Platt scaling on validation, report ECE/ACE, and fix operating thresholds accordingly. Optional deep ensembles or MC dropout provide voxel/edge-level uncertainty used to filter pseudo-labels and to prioritize human proofreading.\par

\medskip
\noindent\textbf{Training protocol (reproducible essentials)}\;
State patch size in physical units (with halo), batch measured in voxels, optimizer (SGD/AdamW), LR schedule (cosine/multistep), mixed precision, gradient accumulation, EMA, and hard-example mining.
Specify positive/negative sampling ratios and class weights to counter extreme imbalance.\par

\medskip
\noindent\textbf{Inference, tiling, and block fusion}\;
Use sliding windows with halo; optional test-time augmentation (in-plane flips/rotations).
Fuse overlaps by averaging or Gaussian weighting; for detection, apply NMS in \emph{physical} radius and deduplicate across blocks; refine masks with optional CRF and reconcile instances across tiles.\par

\medskip
\noindent\textbf{Evaluation protocol (aligned with the chapter)}\;
Detection: PR/AP and \(F_1@r\) under a physical matching radius \(r\) with sensitivity sweeps.
Segmentation: Dice/mIoU, Boundary-F1 with tolerance \(\delta\), plus VOI/Rand \(F\) for instance topology.
Pairing/graph: edge PR/ROC/AUC and calibration curves; report 95\% CIs via bootstrap over instances/tiles.\par

\medskip
\noindent\textbf{Runtime and resources}\;
Report throughput (Mvox/s), peak memory, parameter count/FLOPs, and latency; list hardware and library versions.
Document tile/halo and cross-block reconciliation, as they affect both accuracy and speed.\par

\medskip
\noindent\textbf{Failure modes and mitigations}\;
Low contrast, strong artefacts/ghosting, and polyadic crowding yield misses or merges; mitigations include boundary-head reinforcement, adaptive thresholds, local re-estimation around hard regions, and topology-aware regularizers.\par

\medskip
\noindent\textbf{Interfaces to the rest of the thesis}\;
Losses, matching rules, and calibration follow the definitions in \emph{Problem scope and data regime} and \emph{Benchmarks, metrics \& failure modes} to prevent protocol drift.\par

\medskip
\noindent\textbf{Box D1: encoder--decoder with anisotropic kernels (example)}\;
Backbone with 4--5 levels; alternate in-plane \(k\times k\times 1\) and axial \(1\times 1\times k\); delay the first \(z\)-stride to level \(N\).
Heads: detection \(\hat{Y}^{\text{det}},\hat{O}\), segmentation \(\hat{S}\) and boundary \(\hat{B}\), pairing \(\hat{Z}\) or \(\hat{A}\).\par

\medskip
\noindent\textbf{Algorithm D1: training}\;
\begin{enumerate}
  \item \textbf{Sample}: crop patches (physical units) with halo; stratify hard regions; fix pos/neg ratios and class weights.
  \item \textbf{Augment}: intensity/geometric/noise; if semi-supervised, use weak/strong consistency and filter pseudo-labels.
  \item \textbf{Forward}: backbone \(+\) heads \(\rightarrow \hat{Y}^{\text{det}},\hat{O},\hat{S},\hat{B},\hat{Z}/\hat{A}\).
  \item \textbf{Loss}: compute \(L_{\text{det}},L_{\text{seg}},L_{\text{pair}},L_{\text{topo}}\) and ECE; update with chosen weighting.
  \item \textbf{Optimize}: AdamW/SGD with cosine or multistep LR; mixed precision, accumulation, optional EMA.
  \item \textbf{Select thresholds/temperature}: sweep on validation; fix operating points and calibration temperature.
\end{enumerate}

\noindent\textbf{Algorithm D2: inference}\;
\begin{enumerate}
  \item \textbf{Tiling}: sliding windows with halo; optional in-plane TTA; fuse outputs (mean/Gaussian).
  \item \textbf{Detection}: peak/connected components \(\rightarrow\) NMS (physical radius) and cross-block deduplication.
  \item \textbf{Segmentation}: threshold/optional CRF refinement; instance formation via offsets/energy/region merging.
  \item \textbf{Pairing}: build local cost within radius \(r\) from \(\hat{Z}/\hat{A}\), solve with Hungarian/min-cost flow; filter by contact area/distance.
  \item \textbf{Calibration}: apply temperature \(T\), export PR/ROC and calibration curves with confidence intervals.
\end{enumerate}

\medskip
% ---- Compact tables that fit typical thesis layouts (no extra packages) ----
\begin{table}[t]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{2pt}\renewcommand{\arraystretch}{1.05}
  \begin{tabular}{@{} p{0.19\linewidth} p{0.43\linewidth} p{0.16\linewidth} p{0.20\linewidth} @{}}
    \hline
    \textbf{Archetype} & \textbf{Core design} & \textbf{Pros} & \textbf{Caveats} \\
    \hline
    2D/2.5D U-Net & In-plane conv; slice stacks; axial \(1\times 1\times k\) & Memory/throughput & Limited \(z\)-context \\
    3D U-Net / hybrid & Anisotropic kernels; delayed \(z\)-stride & Strong 3D cues & High memory; complex tiling \\
    CNN+Transformer & Windowed in-plane attn \(+\) axial gating & Large RF; robust & Stability/memory tuning \\
    Multi-head (FPN) & Shared encoder; heads for det/seg/boundary/pairing & End-to-end & Loss balancing \\
    \hline
  \end{tabular}
  \caption{Deep architecture patterns with anisotropy-aware choices.}
  \label{tab:dl-arch-compare}
\end{table}

\begin{table}[t]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{2pt}\renewcommand{\arraystretch}{1.05}
  \begin{tabular}{@{} p{0.23\linewidth} p{0.52\linewidth} p{0.23\linewidth} @{}}
    \hline
    \textbf{Target} & \textbf{Typical loss} & \textbf{Notes} \\
    \hline
    Detection & Focal/weighted BCE \(+\) L1/L2 offsets & Extreme imbalance; tiny objects \\
    Semantic seg. & Dice \(+\) CE/Tversky & Foreground sparsity; calibration \\
    Boundary seg. & Level-set proxy / dist.-transform / Boundary-F1 surrogate & Tolerance band \(\delta\) \\
    Pairing & Contrastive/triplet/InfoNCE or BCE on \(\hat{A}\) & Local soft-assign; global matching at test \\
    Topology & Connectivity/hole/contact proxies & Distance/skeleton approximations \\
    Calibration & Temp. scaling; ECE/ACE & Threshold selection; QA triage \\
    \hline
  \end{tabular}
  \caption{Loss components aligned with the three goals and priors.}
  \label{tab:dl-loss-compare}
\end{table}

\begin{table}[t]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{2pt}\renewcommand{\arraystretch}{1.05}
  \begin{tabular}{@{} p{0.32\linewidth} p{0.66\linewidth} @{}}
    \hline
    \textbf{Category} & \textbf{Examples} \\
    \hline
    Intensity/noise & Brightness/contrast/gamma; histogram match; Poisson/Gaussian; missing-slice sim. \\
    Geometry & In-plane rotations \((0/90/180/270^\circ)\), flips, elastic warps, light scaling \\
    Semi-supervision & Teacher--student weak/strong consistency; conf.-based filtering; morphology checks \\
    Domain adaptation & Histogram/style transfer (lightweight); feature alignment (MMD/adversarial) \\
    \hline
  \end{tabular}
  \caption{Training-time robustness and label-efficiency strategies.}
  \label{tab:dl-aug-compare}
\end{table}

\subsection{Structural priors and constraints}

\noindent\textbf{Scope and taxonomy}\;
We categorize \emph{structural priors/constraints} into four complementary families and indicate where each is enforced:
(i) \emph{biological priors} (membrane contact, thin-sheet thickness, vesicle/mitochondria proximity, polyadic multiplicity);
(ii) \emph{geometric priors} (smoothness, perimeter/curvature control, thickness window, shape compactness);
(iii) \emph{topological priors} (connectivity, no spurious perforations, Euler-characteristics control);
(iv) \emph{graph-level priors} (pre--post edge legality, degree and distance statistics, cross-block consistency).
Realization layers include \underline{differentiable losses}, \underline{differentiable proxies}, \underline{decoding projections/post-processing}, and \underline{post-training regularization}. All distances, radii, areas, and thicknesses are reported in \emph{physical units} (nm or \(\mu\)m).\par

\medskip
\noindent\textbf{Notation and physical units}\;
Let \(\hat S\in[0,1]^{H\times W\times D}\) denote a synapse-probability volume; \(M\in[0,1]^{H\times W\times D}\) a membrane probability or mask; \(\mathrm{Dist}(\cdot)\) a distance transform defined in physical units.
We use \(\rho\) for contact radius, \([\tau_{\min},\tau_{\max}]\) for admissible thickness window, \(A_{\min}\) for minimal contact area, and \(r\) for matching radius (all in nm unless noted).\par

\medskip
\noindent\textbf{Biological context priors}\;
Synaptic interfaces arise at membranes with dense vesicles near presynaptic active zones juxtaposed to postsynaptic densities.
Models benefit from auxiliary heads (membrane affinity, vesicle likelihood, partner-offset vectors) and from rules that allow \emph{polyadic} one-to-many assignments in insects while discouraging artificial one-to-one couplings.
When neuron masks are available, pre/post assignments are constrained to distinct neurites with nonzero membrane contact and bounded centroid distance in physical units.\par

\medskip
\noindent\textbf{Geometric and topological regularization}\;
Thin interfaces are fragile under weak supervision.
Losses typically combine data fidelity with connectivity, hole, and boundary-consistency terms, and enforce hierarchy across vesicles \(\rightarrow\) active zones \(\rightarrow\) synapse masks.
We formalize the components below as differentiable (or differentiably approximated) objectives and provide projection operators for inference-time enforcement.\par

% --------------------------- Loss Set S (formulas) ---------------------------
\medskip
\noindent\textbf{Loss Set S: priors as differentiable (or proxy) objectives}\;

\noindent\textit{S1. Membrane-contact prior}\;
Let \(\mathrm{DT}_M=\mathrm{SignedDist}(M)\) be negative inside membranes.
\[
L_{\mathrm{mc}}
=\frac{1}{|\Omega|}\sum_{\mathbf r}\hat S(\mathbf r)\cdot \max\!\bigl(0,\,\mathrm{DT}_M(\mathbf r)-\rho\bigr),
\qquad
L_{\mathrm{area}}
=\max\!\bigl(0,\,A_{\min}-|\hat S\cap \mathrm{Dilate}(M,\rho)|\bigr).
\]

\noindent\textit{S2. Thin-sheet thickness window}\;
With a soft skeleton \(\mathrm{Skel}(\hat S)\) and thickness estimate \(\mathrm{Thick}(\hat S)\) (via distance transforms):
\[
L_{\mathrm{thick}}
=\frac{1}{|\Omega|}\sum_{\mathbf r}\hat S(\mathbf r)\Bigl[
\max\!\bigl(0,\,\mathrm{Thick}(\hat S)(\mathbf r)-\tau_{\max}\bigr)
+\max\!\bigl(0,\,\tau_{\min}-\mathrm{Thick}(\hat S)(\mathbf r)\bigr)\Bigr].
\]

\noindent\textit{S3. Geometric smoothness (TV/length/curvature)}\;
\[
L_{\mathrm{tv}}=\sum_{\mathbf r}\lVert\nabla \hat S(\mathbf r)\rVert_1,
\qquad
L_{\mathrm{len}}=\sum_{\partial \hat S}1,
\qquad
L_{\mathrm{curv}}\approx \sum_{\partial \hat S}\kappa^2
\ \ (\text{finite-difference approximation}).
\]

\noindent\textit{S4. Topology: connectivity and hole suppression}\;
A soft connectivity proxy (e.g., LogSumExp paths) with representative points \(\mathcal C\):
\[
L_{\mathrm{conn}}=1-\frac{1}{Z}\sum_{c\in\mathcal C}\mathrm{SoftConn}(\hat S;c).
\]
A small-cycle penalty as a proxy to persistent homology:
\[
L_{\mathrm{hole}}\approx \sum_{b\in \text{small-cycles}} w_b\,\sigma\!\bigl(\mathrm{Pers}_b-\epsilon\bigr).
\]

\noindent\textit{S5. Graph-level priors (edge legality and degrees)}\;
With predicted affinity \(\hat A_{ij}\), distance \(d_{ij}\), and membrane-contact gate \(\chi_{ij}\in\{0,1\}\):
\[
L_{\mathrm{edge}}
=\sum_{i,j}\hat A_{ij}\Bigl[\alpha\,\max(0,\,d_{ij}-d_{\max})+\beta\,(1-\chi_{ij})\Bigr],
\qquad
L_{\mathrm{deg}}=\mathrm{KL}\bigl(p_{\mathrm{deg}}^{\mathrm{pred}}\;\|\;p_{\mathrm{deg}}^{\mathrm{prior}}\bigr).
\]

\noindent\textit{S6. Multi-task consistency (membrane/boundary/skeleton)}\;
\[
L_{\mathrm{cons}}
=\lambda_m\,\mathrm{BCE}\!\bigl(\mathbb{1}[\hat S>0.5],\,\mathrm{Dilate}(M,\rho)\bigr)
+\lambda_b\,\mathrm{Dice}\bigl(\hat B,\,\partial \hat S\bigr)
+\lambda_k\,\lVert \mathrm{Skel}(\hat S)-\hat K\rVert_1,
\]
where \(\hat B\) and \(\hat K\) are boundary/skeleton heads.

\noindent\textit{S7. Structured objective (weighted sum)}\;
\begin{eqnarray}
L_{\text{struct}} &=& \lambda_{\mathrm{mc}}L_{\mathrm{mc}}+\lambda_{\mathrm{area}}L_{\mathrm{area}}
+\lambda_{\mathrm{thick}}L_{\mathrm{thick}}+\lambda_{\mathrm{tv}}L_{\mathrm{tv}} \nonumber \\
&&\quad + \lambda_{\mathrm{conn}}L_{\mathrm{conn}}+\lambda_{\mathrm{hole}}L_{\mathrm{hole}}
+\lambda_{\mathrm{edge}}L_{\mathrm{edge}}+\lambda_{\mathrm{deg}}L_{\mathrm{deg}}
+\lambda_{\mathrm{cons}}L_{\mathrm{cons}}.
\end{eqnarray}
Select \(\lambda_{\cdot}\) via grid search, uncertainty weighting, or GradNorm; report sensitivity curves.\par

% --------------------------- Projection / decoding ---------------------------
\medskip
\noindent\textbf{Projection operators (inference-time enforcement)}\;
\begin{enumerate}
  \item \textbf{Small-hole fill} (area \(<\alpha\)); \textbf{Largest-CC} per instance.
  \item \textbf{Thickness clamp} via morphology on a distance field to restrict thickness to \([\tau_{\min},\tau_{\max}]\).
  \item \textbf{Membrane snap}: project non-contact voxels of \(\hat S\) along \(\nabla \mathrm{DT}_M\) to the nearest membrane within \(\rho\); otherwise drop.
  \item \textbf{Edge projection}: cost \(C_{ij}=-\log \hat A_{ij}+\gamma\,\max(0,\,d_{ij}-d_{\max})\); solve a bipartite matching with capacity/degree bounds via Hungarian or min-cost flow.
\end{enumerate}

% --------------------------- Consistency and stability -----------------------
\medskip
\noindent\textbf{Consistency, stability, and gating}\;
Distance/skeleton operators may use precomputed or smoothed proxies; clip gradients for small-cycle terms; warm up with data/geometric losses before increasing topology/graph weights.
Down-weight \(L_{\mathrm{mc}}\) in low-membrane-confidence regions.
For semi-supervision, accept pseudo-labels only if structural violation rates are below a threshold, and increase \(L_{\text{struct}}\) on violations.\par

% --------------------------- Structural compliance metrics -------------------
\medskip
\noindent\textbf{Structural compliance metrics}\;
Beyond Dice/VOI/Rand \(F\), report:
(i) \emph{membrane-contact violation rate} (voxel-/instance-level);
(ii) \emph{thickness out-of-window rate} (mean and 95\% quantiles);
(iii) \emph{connectivity rate} (success of \(s\)--\(t\) connectivity or CC-count distribution);
(iv) \emph{hole rate / Euler error};
(v) \emph{graph legality} (illegal-edge rate, degree-distribution KL, distance-distribution shift).
Attach 95\% confidence intervals via bootstrap over instances/tiles.\par

% --------------------------- Mapping table (compact) -------------------------
\medskip
\begin{table}[t]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{2pt}\renewcommand{\arraystretch}{1.05}
  \begin{tabular}{@{} p{0.20\linewidth} p{0.33\linewidth} p{0.29\linewidth} p{0.16\linewidth} @{}}
    \hline
    \textbf{Prior} & \textbf{Differentiable/approx.\ loss} & \textbf{Projection / postproc.} & \textbf{Notes (extra heads/compute)} \\
    \hline
    Membrane contact & \(L_{\mathrm{mc}}, L_{\mathrm{area}}\) & Snap-to-membrane or prune beyond \(\rho\) & Needs membrane head \(M\) or external mask \\
    Thickness window & \(L_{\mathrm{thick}}\) & Open/close on distance field, clamp to \([\tau_{\min},\tau_{\max}]\) & Distance/skeleton proxy \\
    Smoothness/length & \(L_{\mathrm{tv}}, L_{\mathrm{len}}, L_{\mathrm{curv}}\) & Boundary refinement & Synergy with boundary head \(\hat B\) \\
    Connectivity & \(L_{\mathrm{conn}}\) (soft paths/max-flow proxy) & Keep largest CC & Increase weight gradually \\
    Hole suppression & \(L_{\mathrm{hole}}\) (small cycles) & Fill small holes (\(<\alpha\)) & Enable only on small loops \\
    Graph legality & \(L_{\mathrm{edge}}, L_{\mathrm{deg}}\) & Hungarian/min-cost flow with capacities & Needs \(\hat A\) or embeddings \(\hat Z\) \\
    Multi-task consistency & \(L_{\mathrm{cons}}\) & -- & Needs boundary/skeleton heads \\
    \hline
  \end{tabular}
  \caption{Mapping structural priors to training losses and inference-time projections. All thresholds are in physical units.}
  \label{tab:priors-mapping}
\end{table}

% --------------------------- Failure modes and mitigation --------------------
\medskip
\noindent\textbf{Failure modes and mitigation}\;
Strong anisotropy or low contrast can shift membranes and trigger false contact violations; extremely thin clefts can be over-penalized by thickness terms; crowded polyadic regions may cause tension between connectivity and hole proxies.
Mitigate via confidence-gated \(L_{\mathrm{mc}}\), relaxed thickness windows with data-driven schedules, and local re-estimation around hard regions.\par

\subsection{Benchmarks, metrics \& failure modes}

A rigorous evaluation stack for synapse detection and segmentation should cover: (i) public datasets with fixed splits and annotation granularity; (ii) a metric suite aligned with detection, segmentation, topology, and graph objectives under \emph{unified matching rules}; (iii) engineering indicators (throughput, memory, proofreading cost); and (iv) a failure taxonomy with diagnostic visualizations and protocol-level remedies.

\medskip
\noindent\textbf{Datasets (compact summaries)}\;
Table~\ref{tab:benchmarks-core} and Table~\ref{tab:benchmarks-labels} summarize commonly used benchmarks with a \emph{consistent schema} (modality, physical sampling, volume, label granularity, density, polyadic ratio, license, recommended split). When adding 2024--2025 corpora, extend these tables using the same fields to ensure comparability.

% ---- BM1a: Core facts (kept narrow to fit a page) --------------------------
\medskip
\begin{table}[t]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{2pt}\renewcommand{\arraystretch}{1.05}
  \begin{tabular}{@{} l l l l l @{}}
    \hline
    \textbf{Dataset} & \textbf{Modality / sampling} & \textbf{Voxel size (nm)} & \textbf{Volume (vox)} & \textbf{License \& split} \\
    \hline
    CREMI & ssEM (anisotropic) & \(\Delta_x\times\Delta_y\times\Delta_z\) & \(H\times W\times D\) & public; std.\ train/val/test \\
    FAFB / FlyWire & ssEM, whole brain & \(\Delta_x\times\Delta_y\times\Delta_z\) & full brain; region subsets & public access; region splits \\
    SNEMI3D & ssEM (anisotropic) & \(\Delta_x\times\Delta_y\times\Delta_z\) & single challenge block & challenge split \\
    Lab microvols & ssEM or FIB--SEM & \(\Delta_x\times\Delta_y\times\Delta_z\) & small blocks & per-lab license; ad hoc split \\
    \hline
  \end{tabular}
  \caption{Benchmarks (core). Fill voxel sizes, volumes, and formal licenses in the camera-ready version.}
  \label{tab:benchmarks-core}
\end{table}

% ---- BM1b: Labels and usage (kept narrow to fit a page) --------------------
\medskip
\begin{table}[t]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{2pt}\renewcommand{\arraystretch}{1.05}
  \begin{tabular}{@{} l l l l l @{}}
    \hline
    \textbf{Dataset} & \textbf{Annotations} & \textbf{Syn.\ density (/\,\(\mu\mathrm{m}^3\))} & \textbf{Polyadic (\%)} & \textbf{Primary use} \\
    \hline
    CREMI & cleft masks; pre/post partners & value & value & site/cleft \& pairing validation \\
    FAFB / FlyWire & sites; partner assignments & value & value & large-scale detection \& graph analysis \\
    SNEMI3D & membranes; limited synapses & value & value & thin-structure stress tests \\
    Lab microvols & points/lines/sparse masks & value & value & cross-domain generalization \\
    \hline
  \end{tabular}
  \caption{Benchmarks (labels). Add densities/polyadic ratios and standard citations in the camera-ready version.}
  \label{tab:benchmarks-labels}
\end{table}

\medskip
\noindent\textbf{Box M1: matching \& evaluation protocol (defaults and ranges)}\;
\emph{Detection (points/sites).} One-to-one matching by \emph{physical} distance:
\[
d_{\mathrm{phys}}(p,g)=\big\lVert (\mathbf r_p-\mathbf r_g)\odot \Delta \big\rVert_2 \le r,
\]
processed in descending confidence; default \(r=\textit{XX}\,\mathrm{nm}\) with sensitivity sweeps.  
\emph{Instance masks.} Either (A) 1--1 matching by IoU\(\ge \tau_{\mathrm{IoU}}\) with \(\tau_{\mathrm{IoU}}=0.5\), or (B) center-distance matching with IoU as a quality attribute; \emph{fix one option} and use it consistently.  
\emph{Boundary tolerance.} Boundary-F1 uses a dilation band of radius \(\delta=\textit{YY}\,\mathrm{nm}\).  
\emph{Graph (pairing).} Treat pre--post pairs as edges (state directed/undirected); restrict candidates to a local radius \(r\); training may use soft-assignment, evaluation reports PR/ROC/AUC; edges violating \(d_{\max}\) or membrane contact are counted as negatives.  
\emph{Aggregation.} Report both micro-averages (pool instances) and macro-averages (per-dataset averages), and specify the primary score.

\medskip
\noindent\textbf{Metrics Set B: formulas and implementation notes}\;
\emph{Detection: PR/AP and \(F_1@r\).} With the 1--1 match set \(\mathcal M\),
\[
\mathrm{Precision}=\frac{|\mathcal M|}{N_{\mathrm{pred}}},\quad
\mathrm{Recall}=\frac{|\mathcal M|}{N_{\mathrm{gt}}},\quad
\mathrm{F1@}r=\frac{2\,\mathrm{Prec}\cdot\mathrm{Rec}}{\mathrm{Prec}+\mathrm{Rec}}.
\]
AP is the numerical area under the PR curve (state whether using full integration or 11-point interpolation).  
\emph{Segmentation: Dice / mIoU / Boundary-F1@\(\delta\).}
\[
\mathrm{Dice}=\frac{2|\hat S\cap S|}{|\hat S|+|S|},\qquad
\mathrm{IoU}=\frac{|\hat S\cap S|}{|\hat S\cup S|}.
\]
Let \(B(\cdot)\) be the contour set and \(\mathrm{Dil}_\delta(\cdot)\) a radius-\(\delta\) dilation:
\[
\mathrm{Prec}_b=\frac{|B(\hat S)\cap \mathrm{Dil}_\delta(B(S))|}{|B(\hat S)|},\ \ 
\mathrm{Rec}_b=\frac{|B(S)\cap \mathrm{Dil}_\delta(B(\hat S))|}{|B(S)|},\ \ 
\mathrm{BF1}=\frac{2\,\mathrm{Prec}_b\,\mathrm{Rec}_b}{\mathrm{Prec}_b+\mathrm{Rec}_b}.
\]
For thin interfaces, optionally add \(H_{95}\) (95th-percentile Hausdorff) or ASD.  
\emph{Instance partition: VOI / Rand \(F\).}
\[
\mathrm{VOI}=H(\hat{\mathcal Y}\mid\mathcal Y)+H(\mathcal Y\mid\hat{\mathcal Y}),
\]
with entropies estimated from voxel frequencies; use either Rand \(F\) or ARI (fix one across the thesis).  
\emph{Graph/pairing: edge PR/ROC/AUC and legality.}
\[
\mathrm{Edge\;Prec}=\frac{|\mathcal E^{\mathrm{pred}}\cap \mathcal E^{\mathrm{gt}}|}{|\mathcal E^{\mathrm{pred}}|},\qquad
\mathrm{Edge\;Rec}=\frac{|\mathcal E^{\mathrm{pred}}\cap \mathcal E^{\mathrm{gt}}|}{|\mathcal E^{\mathrm{gt}}|}.
\]
Additionally report illegal-edge rate (distance/contact violations) and degree-distribution divergence (e.g., KL).  
\emph{Calibration: ECE / Brier.} With \(B\) bins,
\[
\mathrm{ECE}=\sum_{b=1}^{B}\frac{n_b}{n}\bigl|\mathrm{acc}(b)-\mathrm{conf}(b)\bigr|,\qquad
\mathrm{Brier}=\frac{1}{n}\sum_{i=1}^n\bigl(\hat p_i-y_i\bigr)^2,
\]
and fix temperature \(T\) by validation scaling.

\medskip
\noindent\textbf{Statistical confidence \& significance}\;
Use stratified bootstrap over instances/tiles (e.g., 1{,}000 resamples) to report 95\% CIs.
For pairwise method comparisons, apply a paired randomization test or Wilcoxon signed-rank on per-tile scores; state the test and \(p\)-value handling in the caption or appendix.

\medskip
\noindent\textbf{Engineering indicators and QA cost}\;
Report (i) \emph{proofreading minutes per \(10^3\) synapses}, estimated from calibrated scores and uncertainty thresholds; (ii) \emph{inference throughput} (vox/s or \(\mu\mathrm{m}^3\)/s) with tile/halo settings; (iii) \emph{peak memory}, parameters/FLOPs, latency; (iv) hardware and library versions. When reporting whole-brain throughput, include block size, overlap, fusion rule, and parallelization scheme.

\medskip
\noindent\textbf{Cross-domain/generalization \& partial labels}\;
For cross-dataset evaluations, stratify by domain factors (region/species/instrument/batch) and keep \((r,\delta,\tau_{\mathrm{IoU}})\) identical across domains.
For incomplete labels, use a \emph{mask-of-interest} to exclude unlabeled regions from TP/FP/FN and \emph{report the number of evaluable instances}.

\medskip
\noindent\textbf{Failure taxonomy F and diagnostics}\;
\emph{Missed detections} (low contrast, faint clefts, dark cytosol) \(\Rightarrow\) mitigate with context aggregation, label/feature augmentation, hard-region mining.  
\emph{Over-/under-segmentation} (membrane adhesion, thin-layer breaks) \(\Rightarrow\) boundary-aware losses, connectivity regularizers, cross-tile instance reconciliation.  
\emph{Block-boundary artefacts} (misalignment, stitching, resampling) \(\Rightarrow\) deformable alignment, overlap inference with consensus merging.  
\emph{Cross-domain shift} (staining/species/region) \(\Rightarrow\) self-supervised pretraining, adaptation, calibrated triage.  
\emph{Partner misassignment} in polyadic neuropil \(\Rightarrow\) offset-vector heads \(+\) neurite-identity constraints; bipartite pruning by distance and membrane contact.  

\medskip
\noindent\textbf{Algorithm E1: standard evaluation pipeline}\;
\begin{enumerate}
  \item \textbf{Preprocess}:\; resample predictions and ground truth to the same \emph{physical} grid; crop to ROI and ignore unlabeled areas.
  \item \textbf{Detection}:\; greedy/Hungarian 1--1 matching under radius \(r\)\(\to\) TP/FP/FN \(\to\) PR/AP and \(F_1@r\).
  \item \textbf{Masks/boundary}:\; compute Dice/mIoU; build a \(\delta\)-band for Boundary-F1.
  \item \textbf{Instances/topology}:\; compute VOI and Rand \(F\) (or ARI); optionally structural compliance (contact/thickness/connectivity/hole rates).
  \item \textbf{Graph/pairing}:\; threshold local candidates into edges; compute edge PR/ROC/AUC, illegal-edge rate, degree KL.
  \item \textbf{Calibration}:\; temperature scaling on validation; fix thresholds; export reliability curves.
  \item \textbf{Confidence}:\; stratified bootstrap (1{,}000 resamples) for 95\% CIs; use paired tests for method deltas.
  \item \textbf{Runtime}:\; record throughput, memory, latency, hardware/libraries; log tile/halo/TTA/fusion settings.
\end{enumerate}

\subsection{Trends and open issues}

This subsection is organized as a four-link template for actionable research: \emph{trend axis} \(\rightarrow\) \emph{key challenge} \(\rightarrow\) \emph{testable hypothesis} \(\rightarrow\) \emph{minimal validation experiment / metrics / risks}. The goal is to pose \emph{falsifiable} and \emph{reproducible} questions, aligned with unified evaluation rules (matching radius \(r\), boundary tolerance \(\delta\), IoU threshold \(\tau_{\mathrm{IoU}}\)) reported in physical units (nm/\(\mu\)m) and consistent with the metrics subsection.

\medskip
\noindent\textbf{Trend A: end-to-end detect--segment--assign}\;
\emph{Key challenge:} reduce cascade error and threshold coupling while optimizing graph-level fidelity. 
\emph{Hypothesis:} training with a differentiable pairing surrogate and a graph-aware loss improves edge PR/AUC and calibration at fixed voxel/mask quality. 
\emph{Minimal experiment:} keep the backbone fixed; compare (i) two-stage cascade versus (ii) joint heads with soft assignment; evaluate at identical tiling/halo and operating points. 
\emph{Primary metrics:} \(F_1@r\) (sites), Dice/mIoU \(\&\) Boundary-F1@\(\delta\) (masks), edge PR/ROC/AUC \(\&\) ECE (graph), plus proofreading minutes per \(10^3\) synapses. 
\emph{Risks/ethics:} over-regularization may hide rare motifs; publish failure cases and calibration plots.

\medskip
\noindent\textbf{Trend B: cross-domain transfer and low-shot learning}\;
\emph{Key challenge:} generalize across species/regions/protocols under sparse labels. 
\emph{Hypothesis:} EM-specific self-supervised pretraining (masked/context) and lightweight feature alignment reduce domain-gap at constant label budget. 
\emph{Minimal experiment:} label-budget curve on target domain (1\%, 2\%, 5\%, 10\%); compare plain supervised vs.\ pretrain+adapt; stratify by domain factors. 
\emph{Primary metrics:} micro/macro \(F_1@r\), Boundary-F1@\(\delta\), VOI/Rand \(F\), edge AUC; report \(\Delta\) (out-of-domain minus in-domain). 
\emph{Risks/ethics:} negative transfer and batch effects; document domain statistics and access approvals.

\medskip
\noindent\textbf{Trend C: uncertainty, calibration, and QA cost}\;
\emph{Key challenge:} small-object imbalance yields miscalibration and volatile thresholds. 
\emph{Hypothesis:} temperature scaling and uncertainty-aware triage minimize expected QA time at fixed accuracy. 
\emph{Minimal experiment:} sweep temperatures \(T\) and thresholds; produce reliability curves and QA-cost frontiers. 
\emph{Primary metrics:} ECE/Brier, PR at fixed QA budget, edge illegal-rate; report 95\% CIs by bootstrap. 
\emph{Risks/ethics:} overly smooth probabilities may defer hard cases; disclose triage heuristics.

\medskip
\noindent\textbf{Trend D: reproducibility, openness, and sustainability}\;
\emph{Key challenge:} heterogeneous protocols impede fair comparison and replication. 
\emph{Hypothesis:} standardized splits, physical-unit tolerances, and containerized pipelines close reproducibility gaps; model distillation/quantization preserves accuracy within \(\leq 1\%\) while doubling throughput. 
\emph{Minimal experiment:} re-run baselines under unified \(r,\delta,\tau_{\mathrm{IoU}}\); report compute/energy and distilled variants. 
\emph{Primary metrics:} primary task scores \(+\) 95\% CIs, throughput (vox/s or \(\mu\mathrm{m}^3\)/s), memory, latency. 
\emph{Risks/ethics:} license compliance and transparent data governance.

\medskip
\noindent\textbf{T--Match: differentiable pairing surrogate (training) and discrete matching (inference)}\;
Let \(\mathcal{P}_{\mathrm{pre}},\mathcal{P}_{\mathrm{post}}\) be pre/post candidates and define a cost
\[
C_{ij} \;=\; \alpha\, d_{\mathrm{phys}}(i,j) \;+\; \beta\,\mathbb{1}\!\left[\neg\text{membrane-contact}\right] \;-\; \log \hat A_{ij},
\]
with \(\hat A_{ij}\) a learned affinity and \(d_{\mathrm{phys}}\) measured in nm. A soft assignment uses an entropically regularized normalization (e.g., Sinkhorn) at temperature \(\tau\):
\[
\mathrm{SoftAssign}_\tau(C)\;=\;\mathrm{Sinkhorn}\!\left(\exp\!\left(-\frac{C}{\tau}\right)\right), 
\qquad
L_{\mathrm{pair}}^{\mathrm{soft}}\;=\;\mathrm{CE}\!\left(\mathrm{SoftAssign}_\tau(C),\,Y\right).
\]
The total training loss couples voxel-, mask-, pair-, structure-, and calibration-terms:
\[
\begin{aligned}
L_{\mathrm{total}}
&=\lambda_{\mathrm{det}} L_{\mathrm{det}}
\;+\;\lambda_{\mathrm{seg}} L_{\mathrm{seg}}
\;+\;\lambda_{\mathrm{pair}} L_{\mathrm{pair}}^{\mathrm{soft}} \\
&\quad
\;+\;\lambda_{\mathrm{topo}} L_{\mathrm{struct}}
\;+\;\lambda_{\mathrm{cal}}\,\mathrm{ECE},
\end{aligned}
\]
with weights tuned on validation. At inference, solve a discrete bipartite matching (Hungarian or min-cost flow) using the \emph{same} cost \(C\); report the soft/hard discrepancy as a percentage.

\medskip
\begin{table}[t]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{1.8pt}\renewcommand{\arraystretch}{1.05}
  \begin{tabular}{@{} p{2.9cm} p{5.2cm} p{4.6cm} p{3.0cm} p{2.8cm} @{}}
    \hline
    \textbf{Trend axis} & \textbf{Falsifiable hypothesis} & \textbf{Minimal experiment} & \textbf{Primary metrics/tests} & \textbf{Risks/ethics} \\
    \hline
    End-to-end pairing & Soft-assignment training improves edge AUC at fixed Dice/BF1@\(\delta\) & Swap cascade head for soft pairing; same backbone/tiling & Edge PR/ROC/AUC, ECE; paired test & Over-regularization of rare motifs \\
    EM pretraining & MAE/contrastive pretrain reduces labels by \(\geq\)X\% for same \(F_1@r\) & Budget curve (1--10\% labels) & AULB, \(F_1@r\), CI & Negative transfer across labs \\
    Structural priors & Adding \(L_{\mathrm{struct}}\) lowers violation rates without hurt to Dice & With/without \(L_{\mathrm{struct}}\) & BF1@\(\delta\), violation rates, Dice & Too-strong priors reduce recall \\
    Semi/active learning & Fixed QA budget \(\Rightarrow\) higher \(F_1@r\) via hard-region mining & Triage by uncertainty+violation & \(F_1@r\), QA min/\(10^3\) & Sampling bias \\
    Synthetic data & Mild domain-rand.\ \(\Rightarrow\) \(+\Delta\) on real set after few-shot & Sim\(\to\)real with few-shot tune & Dice, Edge AUC & Sim-real mismatch \\
    Robustness & Feature alignment \(>\) aug.\ only on OOD & Source\(\to\)multi-target & \(\Delta\) OOD--ID, CI & Added complexity \\
    Calibration & Temperature scaling reduces false edges at fixed recall & Grid over \(T,\tau\) & ECE, PR@QA budget & Over-smoothing \\
    Sustainability & Distill/quantize to \(2\times\) throughput, \(\leq\)1\% drop & KD/8-bit vs.\ full & \(F_1\), throughput, memory & Numeric stability \\
    \hline
  \end{tabular}
  \caption{Trends mapped to hypotheses, experiments, metrics, and risks. Fill concrete numbers, splits, and citations in the camera-ready version; keep tolerances in physical units.}
  \label{tab:trends-matrix}
\end{table}

\medskip
\begin{enumerate}
  \item \textbf{Ablation control:} change one factor only (head/loss/augment/calibration); keep seeds/tiling/halo fixed; report 95\% CIs and paired tests.
  \item \textbf{Sensitivity sweeps:} scan \(r,\delta,\tau_{\mathrm{IoU}}\), temperature \(T\), and \(\tau\) (soft assignment); plot performance--parameter curves.
  \item \textbf{Label-budget curve:} log-scale budget (1\%, 2\%, 5\%, 10\%, \dots); report area-under-label-budget (AULB).
  \item \textbf{In-/out-of-domain stratification:} same operating point; report ID/OOD scores and \(\Delta\); include domain statistics.
  \item \textbf{Structural compliance:} add membrane/thickness/connectivity/hole \emph{violation rates} to main results with 95\% CIs.
  \item \textbf{Engineering ledger:} throughput, memory, params/FLOPs, latency/energy (if available); record TTA/halo/fusion strategies.
\end{enumerate}